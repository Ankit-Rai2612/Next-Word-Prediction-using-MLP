{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ankit-Rai2612/Next-Word-Prediction-using-MLP/blob/main/Next_Word_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT_JJW11iUjg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22230b88-9c2d-42dd-8494-e750e27e9375"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "PyTorch version: 2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')\n",
        "print(f'PyTorch version: {torch.__version__}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'CUDA device name: {torch.cuda.get_device_name(0)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nhA6pmfpDXN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a20197-5183-4eff-fa6a-d0e5044b8b08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IwBTlfEpJEH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/Next-Word-Generator\"\n",
        "\n",
        "assets_path = os.path.join(base_path, \"assets\")\n",
        "model_path = os.path.join(base_path, \"models/mlp/model_context_5_emb_32_act_leaky_relu_seed_42.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4wnn99jpLBi"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(os.path.join(assets_path, \"word_to_index.json\"), \"r\") as f:\n",
        "    word_to_index = json.load(f)\n",
        "\n",
        "with open(os.path.join(assets_path, \"index_to_word.json\"), \"r\") as f:\n",
        "    index_to_word = json.load(f)\n",
        "    index_to_word = {int(k): v for k, v in index_to_word.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EanEtBzYMwe",
        "outputId": "0836afd2-492d-4aa2-9631-f06fd16ea12a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNPLrAb9pK-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b71d0af2-8c80-4447-944e-f0347f852e91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NextWordMLP(\n",
            "  (embedding): Embedding(14343, 100)\n",
            "  (fc1): Linear(in_features=500, out_features=256, bias=True)\n",
            "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout1): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=256, out_features=14343, bias=True)\n",
            "  (activation_function): ReLU()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NextWordMLP(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_rate, context_size, activation_function):\n",
        "        super(NextWordMLP, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.fc1 = nn.Linear(embedding_dim * context_size, hidden_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).view(x.size(0), -1)\n",
        "        x = self.dropout1(self.activation_function(self.bn1(self.fc1(x))))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "# vocab_size = len(vocab)\n",
        "vocab_size = len(word_to_index)\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_dim = 256\n",
        "dropout_rate = 0.5\n",
        "context_size = 5\n",
        "activation_function = nn.ReLU()\n",
        "base= NextWordMLP(vocab_size, embedding_dim, hidden_dim, dropout_rate, context_size, activation_function)\n",
        "print(base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXhg4QtopTZc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a27423fd-40cc-4a10-fff6-36695e9e58d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded successfully from Google Drive\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "activation_function_map = {\n",
        "    \"tanh\": torch.tanh,\n",
        "    \"relu\": F.relu,\n",
        "    \"leaky_relu\": F.leaky_relu\n",
        "}\n",
        "activation_function = activation_function_map[\"leaky_relu\"]\n",
        "\n",
        "context_size = 5\n",
        "embedding_dim = 32\n",
        "seed = 42\n",
        "\n",
        "model = NextWordMLP(\n",
        "    vocab_size=len(word_to_index),\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_dim=1024,\n",
        "    dropout_rate=0.3,\n",
        "    context_size=context_size,\n",
        "    activation_function=activation_function\n",
        ").to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval()\n",
        "print(\"✅ Model loaded successfully from Google Drive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuIrtvGJpTWv"
      },
      "outputs": [],
      "source": [
        "def words_to_indices(words, word_to_index):\n",
        "    return [word_to_index[word] if word in word_to_index else word_to_index['pad'] for word in words]\n",
        "\n",
        "def generate_text(model, start_sequence, num_words, temperature=1.0):\n",
        "    model.eval()\n",
        "    generated = list(start_sequence)\n",
        "    for _ in range(num_words):\n",
        "        input_seq = torch.tensor(generated[-context_size:], dtype=torch.long).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(input_seq)\n",
        "        logits = output.squeeze(0) / temperature\n",
        "        next_word_idx = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).item()\n",
        "        generated.append(next_word_idx)\n",
        "        if index_to_word[next_word_idx] == 'end':\n",
        "            break\n",
        "    return ' '.join(index_to_word[idx] for idx in generated if index_to_word[idx] != 'pad')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ivn6iunhuCX"
      },
      "outputs": [],
      "source": [
        "def clean_and_tokenize(text):\n",
        "    if text is None or text.strip() == \"\":\n",
        "        return []\n",
        "\n",
        "\n",
        "    text = text.replace('start', ' start ').replace('end', ' end ')\n",
        "\n",
        "    text = re.sub(r'([.,!?])', r' \\1 ', text)  # Add spaces around punctuation marks\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra spaces\n",
        "    segments = text.lower().split() # Split text into segments\n",
        "\n",
        "    return segments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww1ngUHEmFB2"
      },
      "source": [
        "# Testing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMKmF9tOpK8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23795c1a-6628-47e9-f559-976fe299e9a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Recipe: mix milk and cream , the spices also . attach the cooked and on the gravy cooked . after the onion becomes soft , add in ginger , green chillies , curry leaves and saute for about 3 minutes on medium flame , so the roasted potatoes , sprinkle some water and knead for a minute till they turn golden brown and drain the excess water and keep it aside . now make a baking sheet on both sides and allow the end\n"
          ]
        }
      ],
      "source": [
        "# Example input\n",
        "start_sequence_words = \"Mix milk and cream\"\n",
        "start_sequence_words = clean_and_tokenize(start_sequence_words)  # <-- must define same tokenizer\n",
        "start_sequence_indices = words_to_indices(start_sequence_words, word_to_index)\n",
        "\n",
        "if len(start_sequence_indices) < context_size:\n",
        "    start_sequence_indices = [word_to_index['pad']] * (context_size - len(start_sequence_indices)) + start_sequence_indices\n",
        "\n",
        "generated_text = generate_text(model, start_sequence_indices, num_words=100, temperature=1.0)\n",
        "print(\"Generated Recipe:\", generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B43xRd8CiiRc"
      },
      "source": [
        "### Testing with new random inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoAATyhmpK5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4e52610-41c1-4371-cceb-eb4a62d6dd2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Recipe: my name is to drain the water . refresh the vegetables into the\n"
          ]
        }
      ],
      "source": [
        "# Example input\n",
        "start_sequence_words = \"My name is\" # New input\n",
        "start_sequence_words = clean_and_tokenize(start_sequence_words)  # <-- must define same tokenizer\n",
        "start_sequence_indices = words_to_indices(start_sequence_words, word_to_index)\n",
        "\n",
        "if len(start_sequence_indices) < context_size:\n",
        "    start_sequence_indices = [word_to_index['pad']] * (context_size - len(start_sequence_indices)) + start_sequence_indices\n",
        "\n",
        "generated_text = generate_text(model, start_sequence_indices, num_words=10, temperature=1.0) # This time predicting 10 words. Earlier 100 words were predicted\n",
        "print(\"Generated Recipe:\", generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db3z2lRVjBWm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2246b730-fec2-4b0a-da60-00e6cb34fde9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Recipe: good , turn off the heat and serve . serve puli engi with some curry and serve the biryani with a\n"
          ]
        }
      ],
      "source": [
        "# Example input\n",
        "start_sequence_words = \"Good\" # New input\n",
        "start_sequence_words = clean_and_tokenize(start_sequence_words)  # <-- must define same tokenizer\n",
        "start_sequence_indices = words_to_indices(start_sequence_words, word_to_index)\n",
        "\n",
        "if len(start_sequence_indices) < context_size:\n",
        "    start_sequence_indices = [word_to_index['pad']] * (context_size - len(start_sequence_indices)) + start_sequence_indices\n",
        "\n",
        "generated_text = generate_text(model, start_sequence_indices, num_words=20, temperature=1.0) # This time predicting 10 words. Earlier 100 words were predicted\n",
        "print(\"Generated Recipe:\", generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqRzcG1Dje1A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a02b00e7-3a53-41ca-f9fe-aa9c2b0b138c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Recipe: this takes about 3 to 4 minutes . now add 3 cups of water and place it in a bowl . set aside\n"
          ]
        }
      ],
      "source": [
        "# Example input\n",
        "start_sequence_words = \"This takes about\" # New input\n",
        "start_sequence_words = clean_and_tokenize(start_sequence_words)  # <-- must define same tokenizer\n",
        "start_sequence_indices = words_to_indices(start_sequence_words, word_to_index)\n",
        "\n",
        "if len(start_sequence_indices) < context_size:\n",
        "    start_sequence_indices = [word_to_index['pad']] * (context_size - len(start_sequence_indices)) + start_sequence_indices\n",
        "\n",
        "generated_text = generate_text(model, start_sequence_indices, num_words=20, temperature=1.0) # This time predicting 10 words. Earlier 100 words were predicted\n",
        "print(\"Generated Recipe:\", generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQiWpERlj6nn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aae71de4-ef53-4975-cc24-bc2294b37784"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Recipe: to begin making the ragi malpua rabri recipe , mint & cucumber raita . end\n"
          ]
        }
      ],
      "source": [
        "# Example input\n",
        "start_sequence_words = \"To begin making the Ragi\" # New input\n",
        "start_sequence_words = clean_and_tokenize(start_sequence_words)  # <-- must define same tokenizer\n",
        "start_sequence_indices = words_to_indices(start_sequence_words, word_to_index)\n",
        "\n",
        "if len(start_sequence_indices) < context_size:\n",
        "    start_sequence_indices = [word_to_index['pad']] * (context_size - len(start_sequence_indices)) + start_sequence_indices\n",
        "\n",
        "generated_text = generate_text(model, start_sequence_indices, num_words=20, temperature=1.0) # This time predicting 10 words. Earlier 100 words were predicted\n",
        "print(\"Generated Recipe:\", generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXRENSOPkFsg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc8b0c82-038c-48cd-ef22-ba6a5398b259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Recipe: to make tomato puliogere , first cut the tomatoes into thin roundels . in a large mixing bowl or a bowl , whisk them up . add\n"
          ]
        }
      ],
      "source": [
        "# Example input\n",
        "start_sequence_words = \"To make tomato puliogere, first cut\" # New input\n",
        "start_sequence_words = clean_and_tokenize(start_sequence_words)  # <-- must define same tokenizer\n",
        "start_sequence_indices = words_to_indices(start_sequence_words, word_to_index)\n",
        "\n",
        "if len(start_sequence_indices) < context_size:\n",
        "    start_sequence_indices = [word_to_index['pad']] * (context_size - len(start_sequence_indices)) + start_sequence_indices\n",
        "\n",
        "generated_text = generate_text(model, start_sequence_indices, num_words=20, temperature=1.0) # This time predicting 10 words. Earlier 100 words were predicted\n",
        "print(\"Generated Recipe:\", generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tzG-QiPkjOl"
      },
      "source": [
        "#### Good output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2EJ8ug9kaEU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7faf8442-8bee-4803-c0ba-56501590c4c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Recipe: to make andhra style alam pachadi , first heat oil in each of the cavities . place this deep after your dessert for at least indian week on the grill . once its and serve the rice . soak dates in water for 15-20 minutes . drain the water from the rice , add the grated coconut , mustard oil , sugar and add the rice powder . it will thicken , once the milk comes to a boil , add cumin seeds , curry leaves and broken mor milagai . add oil in a heavy bottomed pan . add the chopped vegetables to the mashed\n"
          ]
        }
      ],
      "source": [
        "# Example input\n",
        "start_sequence_words = \"To make Andhra Style Alam Pachadi\" # New input\n",
        "start_sequence_words = clean_and_tokenize(start_sequence_words)  # <-- must define same tokenizer\n",
        "start_sequence_indices = words_to_indices(start_sequence_words, word_to_index)\n",
        "\n",
        "if len(start_sequence_indices) < context_size:\n",
        "    start_sequence_indices = [word_to_index['pad']] * (context_size - len(start_sequence_indices)) + start_sequence_indices\n",
        "\n",
        "generated_text = generate_text(model, start_sequence_indices, num_words=100, temperature=1.0) # This time predicting 10 words. Earlier 100 words were predicted\n",
        "print(\"Generated Recipe:\", generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdAwLpq6knMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b0fb59-e54d-45c9-db22-4d423963a44d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Recipe: boil the water first to form a smooth paste , adding water little at a time and whisk until the butter is bl end\n"
          ]
        }
      ],
      "source": [
        "# Example input\n",
        "start_sequence_words = \"Boil the water first\" # New input\n",
        "start_sequence_words = clean_and_tokenize(start_sequence_words)  # <-- must define same tokenizer\n",
        "start_sequence_indices = words_to_indices(start_sequence_words, word_to_index)\n",
        "\n",
        "if len(start_sequence_indices) < context_size:\n",
        "    start_sequence_indices = [word_to_index['pad']] * (context_size - len(start_sequence_indices)) + start_sequence_indices\n",
        "\n",
        "generated_text = generate_text(model, start_sequence_indices, num_words=100, temperature=1.0) # This time predicting 10 words. Earlier 100 words were predicted\n",
        "print(\"Generated Recipe:\", generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQuhdo1DlgfB"
      },
      "source": [
        "### Testing with user inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0fjhjw9ljNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29ea60ab-d7b1-445e-cc8c-b446d006f97f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the input string\n",
            "This is \n",
            "Enter the number of words to predict\n",
            "5\n",
            "Generated Recipe: this is over to cook , add\n"
          ]
        }
      ],
      "source": [
        "# Example input\n",
        "print(\"Enter the input string\")\n",
        "start_sequence_words = input() # New input\n",
        "print(\"Enter the number of words to predict\")\n",
        "num_words = int(input())\n",
        "start_sequence_words = clean_and_tokenize(start_sequence_words)  # <-- must define same tokenizer\n",
        "start_sequence_indices = words_to_indices(start_sequence_words, word_to_index)\n",
        "\n",
        "if len(start_sequence_indices) < context_size:\n",
        "    start_sequence_indices = [word_to_index['pad']] * (context_size - len(start_sequence_indices)) + start_sequence_indices\n",
        "\n",
        "generated_text = generate_text(model, start_sequence_indices, num_words=num_words, temperature=1.0) # This time predicting 10 words. Earlier 100 words were predicted\n",
        "print(\"Generated Recipe:\", generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oyfbk6XFipOf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}